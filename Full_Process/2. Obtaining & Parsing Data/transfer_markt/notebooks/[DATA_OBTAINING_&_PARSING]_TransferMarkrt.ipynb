{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriestlyAdejo/InstagramAppClone/blob/main/Full_Process/2.%20Obtaining%20%26%20Parsing%20Data/transfer_markt/notebooks/%5BDATA_OBTAINING_%26_PARSING%5D_TransferMarkrt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Notebook Setup"
      ],
      "metadata": {
        "id": "Zjk1Chy4bJb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting-up Utility Class"
      ],
      "metadata": {
        "id": "hK90Yj3pbN13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf smaple_data\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "def save_dataframes_to_pickle(dataframes_dict, directory):\n",
        "    # Change to home directory - need to make this the home project directory on user machine!\n",
        "    os.chdir(\"/content\")\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # Define the path to the pickle file\n",
        "    pickle_file_path = os.path.join(directory, 'all_transfer_markt_data.pickle')\n",
        "\n",
        "    with open(pickle_file_path, 'wb') as f:\n",
        "        pickle.dump(dataframes_dict, f)\n",
        "\n",
        "def save_metadata_to_file(dataframes_dict, directory):\n",
        "    # Change to home directory - need to make this the home project directory on user machine!\n",
        "    os.chdir(\"/content\")\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    for k, v in dataframes_dict.items():\n",
        "      # Make second loop\n",
        "\n",
        "      dataset_df = v[\"dataset_df\"]\n",
        "      schema_df = v[\"schema_df\"]\n",
        "      schema_str = v[\"schema_str\"]\n",
        "\n",
        "      # Create the directory if it doesn't exist\n",
        "      if not os.path.exists(f\"{directory}/{k}\"):\n",
        "          os.makedirs(f\"{directory}/{k}\")\n",
        "\n",
        "      # Making file paths\n",
        "      dataset_df_file_path = os.path.join(f\"{directory}/{k}\", f\"dataset_df.json\")\n",
        "      schema_df_file_path = os.path.join(f\"{directory}/{k}\", f\"schema_df.json\")\n",
        "      schema_str_file_path = os.path.join(f\"{directory}/{k}\", f\"schema_str.txt\")\n",
        "\n",
        "      with open(dataset_df_file_path, 'w') as file:\n",
        "          json.dump(dataset_df.to_json(), file)\n",
        "\n",
        "      with open(schema_df_file_path, 'w') as file:\n",
        "          json.dump(schema_df.to_json(), file)\n",
        "\n",
        "      with open(schema_str_file_path, 'w') as file:\n",
        "          file.write(schema_str)\n",
        "\n",
        "# Example usage\n",
        "datasets = load_all_datasets(all_dataset_names, json_data)\n",
        "save_dataframes_to_pickle(datasets, 'transfer_markt_exported_data')\n",
        "save_metadata_to_file(datasets, 'transfer_markt_exported_data')\n",
        "\n",
        "\n",
        "# Loading back files\n",
        "def get_pickle_at(file_loc):\n",
        "    objects = []\n",
        "    with (open(file_loc, \"rb\")) as openfile:\n",
        "        while True:\n",
        "            try:\n",
        "                objects.append(pickle.load(openfile))\n",
        "            except EOFError:\n",
        "                break\n",
        "    return objects\n",
        "\n",
        "\n",
        "!du -h --max-depth=5 \"transfer_markt_exported_data\"\n",
        "import subprocess\n",
        "\n",
        "def get_directory_sizes(path, max_depth=5):\n",
        "    result = subprocess.run(['du', '-h', f'--max-depth={max_depth}', path], stdout=subprocess.PIPE)\n",
        "    output = result.stdout.decode('utf-8').strip()\n",
        "    lines = output.split('\\n')\n",
        "    sizes = [line.split('\\t') for line in lines]\n",
        "    return sizes\n",
        "\n",
        "directory_path = 'transfer_markt_exported_data'\n",
        "directory_sizes = get_directory_sizes(directory_path)\n",
        "\n",
        "# Print the sizes\n",
        "for size in directory_sizes:\n",
        "    print(size)\n",
        "\n",
        "print(\"Total size:\", directory_sizes[-1][0])\n",
        "\n",
        "\n",
        "\n",
        "objects = get_pickle_at(\"/content/exported_data/all_transfer_markt_data.pickle\")\n",
        "\n",
        "github_user_name = \"PriestlyAdejo\"\n",
        "repo_name = \"beat_the_bookie\"\n",
        "# Need to put into github secrets\n",
        "PERSONAL_ACCESS_TOKEN = \"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\"\n",
        "!git config --global user.email 'zcemdej@ucl.ac.uk'\n",
        "!git config --global user.name 'PriestlyAdejo'\n",
        "\n",
        "#Make a clone of github REPO\n",
        "git_string = f\"https://{github_user_name}:{PERSONAL_ACCESS_TOKEN}@github.com/{github_user_name}/{repo_name}.git\"\n",
        "!git clone {git_string}\n",
        "\n",
        "# Copy file from either google drive after mounting using file browser\n",
        "# !cp <PATH_OF_FILE_TO_COPY> /content/<REPO_NAME>\n",
        "\n",
        "\n",
        "os.chdir(\"/content/beat_the_bookie\")\n",
        "\n",
        "\n",
        "#Download git-lfs to Push Files larger than 100MB.\n",
        "!wget -O git-lfs.tar.gz https://github.com/git-lfs/git-lfs/releases/download/v2.13.2/git-lfs-linux-amd64-v2.13.2.tar.gz\n",
        "!tar xzf git-lfs.tar.gz\n",
        "!bash ./install.sh\n",
        "!git lfs install\n",
        "%cd repo_name\n",
        "\n",
        "#FILE_NAME is the file with size >100MB and you wants to PUSH to GITHUB\n",
        "!git lfs track <FILE_NAME>\n",
        "\n",
        "!git status\n",
        "\n",
        "!git add <FILE_NAME>\n",
        "!git commit -m 'commit message'  # commit in Colab\n",
        "!git push\n",
        "\n"
      ],
      "metadata": {
        "id": "GQn7ks9xbI0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import shutil\n",
        "import subprocess\n",
        "from typing import List, Dict\n",
        "\n",
        "class GitHubManager:\n",
        "    def __init__(self, project_directory: str, github_username: str, github_email: str, repo_name: str, personal_access_token: str):\n",
        "        self.project_directory = project_directory\n",
        "        self.github_username = github_username\n",
        "        self.github_email = github_email\n",
        "        self.repo_name = repo_name\n",
        "        self.personal_access_token = personal_access_token\n",
        "        self.debug = True\n",
        "\n",
        "        # Always make sure initial working directory is home\n",
        "        os.chdir(self.project_directory)\n",
        "\n",
        "        # Configure Git\n",
        "        self.run_command(f\"git config --global user.email '{self.github_email}'\")\n",
        "        self.run_command(f\"git config --global user.name '{self.github_username}'\")\n",
        "\n",
        "        # Clone repository\n",
        "        self.clone_github_repo()\n",
        "        self.cloned = True\n",
        "\n",
        "    def run_command(self, command):\n",
        "        try:\n",
        "            result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            output = result.stdout.decode('utf-8').strip()\n",
        "            error = result.stderr.decode('utf-8').strip()\n",
        "\n",
        "            if self.debug:\n",
        "                # Check if output or error is empty and print accordingly\n",
        "                print(f\"Executing command: {command}\")\n",
        "                print(\"Output:\\n\", f\"{output}\\n\" if output else \" None\")\n",
        "                print(\"Error:\\n\", f\"{error}\\n\" if error else \" None\\n\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error executing command: {e}\")\n",
        "\n",
        "    def install_git_lfs(self) -> None:\n",
        "        self.run_command(\"wget -O git-lfs.tar.gz https://github.com/git-lfs/git-lfs/releases/download/v2.13.2/git-lfs-linux-amd64-v2.13.2.tar.gz\")\n",
        "        self.run_command(\"tar xzf git-lfs.tar.gz\")\n",
        "        self.run_command(\"bash ./install.sh\")\n",
        "        self.run_command(\"git lfs install\")\n",
        "\n",
        "    def clone_github_repo(self):\n",
        "        os.chdir(self.project_directory)\n",
        "\n",
        "        repo_path = os.path.join(self.project_directory, self.repo_name)\n",
        "\n",
        "        # Check if the directory exists and delete it\n",
        "        if os.path.exists(repo_path):\n",
        "            shutil.rmtree(repo_path)\n",
        "\n",
        "        try:\n",
        "            git_string = f\"https://{self.github_username}:{self.personal_access_token}@github.com/{self.github_username}/{self.repo_name}.git\"\n",
        "            clone_command = f\"git clone {git_string} {repo_path}\"\n",
        "            self.run_command(clone_command)\n",
        "            os.chdir(repo_path)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error cloning repository: {e}\")\n",
        "\n",
        "    def git_status(self):\n",
        "        self.run_command(\"git status\")\n",
        "\n",
        "    def git_add(self, directory):\n",
        "        self.run_command(f\"git add {directory}\")\n",
        "\n",
        "    def git_pull(self):\n",
        "        self.run_command(\"git pull origin main\")\n",
        "\n",
        "    def git_commit(self, msg):\n",
        "        self.run_command(f\"git commit -m \\\"{msg}\\\"\")\n",
        "\n",
        "    def git_push(self):\n",
        "        self.run_command(\"git push\")\n",
        "\n",
        "    def git_lfs_track(self, file_name):\n",
        "        self.run_command(f\"git lfs track \\\"{file_name}\\\"\")\n",
        "\n",
        "\n",
        "class DataManagementUtility(GitHubManager):\n",
        "    def __init__(self, project_directory: str, github_username: str, github_email: str, repo_name: str, personal_access_token: str):\n",
        "        super().__init__(project_directory, github_username, github_email, repo_name, personal_access_token)\n",
        "\n",
        "        # Clone repository and install large file storage (for now)\n",
        "        if self.cloned:\n",
        "          self.install_git_lfs()\n",
        "\n",
        "    def save_dict_to_pickle(self, dataframes_dict: Dict, directory: str) -> None:\n",
        "        pickle_file_path = os.path.join(self.project_directory, directory, 'all_data.pickle')\n",
        "        os.makedirs(os.path.dirname(pickle_file_path), exist_ok=True)\n",
        "\n",
        "        with open(pickle_file_path, 'wb') as f:\n",
        "            pickle.dump(dataframes_dict, f)\n",
        "        return pickle_file_path\n",
        "\n",
        "    def save_metadata_to_file(self, dataframes_dict: Dict, directory: str) -> None:\n",
        "        dir_paths = []\n",
        "        for key, value in dataframes_dict.items():\n",
        "            dir_path = os.path.join(self.project_directory, directory, key)\n",
        "            dir_paths.append(dir_path)\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "            # Save dataset and schema dataframes and text\n",
        "            for df_type, df in value.items():\n",
        "                if not \"_str\" in df_type:\n",
        "                    df.to_json(os.path.join(dir_path, f\"{df_type}.json\"))\n",
        "                else:\n",
        "                    with open(dir_path + f\"{df_type}.txt\", 'w') as file:\n",
        "                        file.write(df)\n",
        "            return dir_paths\n",
        "\n",
        "    def load_dict_from_pickle(self, pickle_path: str) -> List:\n",
        "        objects = []\n",
        "        with (open(pickle_path, \"rb\")) as openfile:\n",
        "            while True:\n",
        "                try:\n",
        "                    objects.append(pickle.load(openfile))\n",
        "                except EOFError:\n",
        "                    break\n",
        "        return objects\n",
        "\n",
        "    # Needs to take specific directory to clone files into\n",
        "    def track_large_files(self, file_names: List[str]) -> None:\n",
        "        if file_names:\n",
        "          os.chdir(os.path.join(self.project_directory, self.repo_name))\n",
        "          for file_name in file_names:\n",
        "              self.run_command(f\"git lfs track {file_name}\")\n",
        "\n",
        "    def get_large_files(self, path: str, size_threshold: str = '100M') -> List[str]:\n",
        "        \"\"\"\n",
        "        Finds and returns a list of files larger than the specified size threshold.\n",
        "        Size threshold is a string like '100M' for 100 Megabytes.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            cmd = ['find', path, '-size', '+' + size_threshold, '-print']\n",
        "            result = subprocess.run(cmd, stdout=subprocess.PIPE, check=True)\n",
        "            output = result.stdout.decode('utf-8').strip()\n",
        "            return output.split('\\n') if output else []\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error executing command: {e}\")\n",
        "            return []\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Command not found. Are you running this on a Unix-like system?\")\n",
        "            return []\n"
      ],
      "metadata": {
        "id": "r3urgz9aewnl"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PERSONAL_ACCESS_TOKEN = \"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\"\n",
        "utils = DataManagementUtility(\"/content\", \"PriestlyAdejo\", \"zcemdej@ucl.ac.uk\", \"beat_the_bookie\", PERSONAL_ACCESS_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIH2Z1RSAWb-",
        "outputId": "ba38c2e3-bd5f-4992-fbd9-af630a0bec1b"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing command: git config --global user.email 'zcemdej@ucl.ac.uk'\n",
            "Output:\n",
            "  None\n",
            "Error:\n",
            "  None\n",
            "\n",
            "Executing command: git config --global user.name 'PriestlyAdejo'\n",
            "Output:\n",
            "  None\n",
            "Error:\n",
            "  None\n",
            "\n",
            "Executing command: git clone https://PriestlyAdejo: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@github.com/PriestlyAdejo/beat_the_bookie.git /content/beat_the_bookie\n",
            "Output:\n",
            "  None\n",
            "Error:\n",
            " Cloning into '/content/beat_the_bookie'...\n",
            "\n",
            "Executing command: wget -O git-lfs.tar.gz https://github.com/git-lfs/git-lfs/releases/download/v2.13.2/git-lfs-linux-amd64-v2.13.2.tar.gz\n",
            "Output:\n",
            "  None\n",
            "Error:\n",
            " --2023-12-07 11:33:24--  https://github.com/git-lfs/git-lfs/releases/download/v2.13.2/git-lfs-linux-amd64-v2.13.2.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/13021798/31608d80-55cd-11eb-90aa-129d4821d135?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231207%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231207T113324Z&X-Amz-Expires=300&X-Amz-Signature=173542a088c99a38b2a97eb63b820f225a1d2a2411165de939096d5dc31da730&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=13021798&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-v2.13.2.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-12-07 11:33:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/13021798/31608d80-55cd-11eb-90aa-129d4821d135?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231207%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231207T113324Z&X-Amz-Expires=300&X-Amz-Signature=173542a088c99a38b2a97eb63b820f225a1d2a2411165de939096d5dc31da730&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=13021798&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-v2.13.2.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4743788 (4.5M) [application/octet-stream]\n",
            "Saving to: ‘git-lfs.tar.gz’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  1% 2.96M 2s\n",
            "    50K .......... .......... .......... .......... ..........  2% 3.09M 1s\n",
            "   100K .......... .......... .......... .......... ..........  3% 4.86M 1s\n",
            "   150K .......... .......... .......... .......... ..........  4% 17.0M 1s\n",
            "   200K .......... .......... .......... .......... ..........  5% 19.0M 1s\n",
            "   250K .......... .......... .......... .......... ..........  6% 25.0M 1s\n",
            "   300K .......... .......... .......... .......... ..........  7% 6.50M 1s\n",
            "   350K .......... .......... .......... .......... ..........  8% 29.5M 1s\n",
            "   400K .......... .......... .......... .......... ..........  9% 37.6M 1s\n",
            "   450K .......... .......... .......... .......... .......... 10% 34.3M 1s\n",
            "   500K .......... .......... .......... .......... .......... 11% 57.4M 0s\n",
            "   550K .......... .......... .......... .......... .......... 12% 67.5M 0s\n",
            "   600K .......... .......... .......... .......... .......... 14% 7.43M 0s\n",
            "   650K .......... .......... .......... .......... .......... 15% 30.4M 0s\n",
            "   700K .......... .......... .......... .......... .......... 16% 86.1M 0s\n",
            "   750K .......... .......... .......... .......... .......... 17% 37.9M 0s\n",
            "   800K .......... .......... .......... .......... .......... 18% 49.3M 0s\n",
            "   850K .......... .......... .......... .......... .......... 19% 51.4M 0s\n",
            "   900K .......... .......... .......... .......... .......... 20% 55.5M 0s\n",
            "   950K .......... .......... .......... .......... .......... 21% 51.1M 0s\n",
            "  1000K .......... .......... .......... .......... .......... 22% 44.5M 0s\n",
            "  1050K .......... .......... .......... .......... .......... 23% 58.3M 0s\n",
            "  1100K .......... .......... .......... .......... .......... 24% 59.4M 0s\n",
            "  1150K .......... .......... .......... .......... .......... 25% 60.2M 0s\n",
            "  1200K .......... .......... .......... .......... .......... 26% 53.5M 0s\n",
            "  1250K .......... .......... .......... .......... .......... 28% 18.7M 0s\n",
            "  1300K .......... .......... .......... .......... .......... 29% 90.2M 0s\n",
            "  1350K .......... .......... .......... .......... .......... 30% 86.1M 0s\n",
            "  1400K .......... .......... .......... .......... .......... 31% 51.8M 0s\n",
            "  1450K .......... .......... .......... .......... .......... 32% 51.4M 0s\n",
            "  1500K .......... .......... .......... .......... .......... 33% 59.5M 0s\n",
            "  1550K .......... .......... .......... .......... .......... 34% 57.3M 0s\n",
            "  1600K .......... .......... .......... .......... .......... 35% 82.4M 0s\n",
            "  1650K .......... .......... .......... .......... .......... 36%  219M 0s\n",
            "  1700K .......... .......... .......... .......... .......... 37%  260M 0s\n",
            "  1750K .......... .......... .......... .......... .......... 38%  267M 0s\n",
            "  1800K .......... .......... .......... .......... .......... 39%  260M 0s\n",
            "  1850K .......... .......... .......... .......... .......... 41%  234M 0s\n",
            "  1900K .......... .......... .......... .......... .......... 42%  241M 0s\n",
            "  1950K .......... .......... .......... .......... .......... 43%  227M 0s\n",
            "  2000K .......... .......... .......... .......... .......... 44%  265M 0s\n",
            "  2050K .......... .......... .......... .......... .......... 45%  237M 0s\n",
            "  2100K .......... .......... .......... .......... .......... 46%  268M 0s\n",
            "  2150K .......... .......... .......... .......... .......... 47%  205M 0s\n",
            "  2200K .......... .......... .......... .......... .......... 48%  267M 0s\n",
            "  2250K .......... .......... .......... .......... .......... 49%  247M 0s\n",
            "  2300K .......... .......... .......... .......... .......... 50%  238M 0s\n",
            "  2350K .......... .......... .......... .......... .......... 51%  210M 0s\n",
            "  2400K .......... .......... .......... .......... .......... 52%  255M 0s\n",
            "  2450K .......... .......... .......... .......... .......... 53%  271M 0s\n",
            "  2500K .......... .......... .......... .......... .......... 55% 10.0M 0s\n",
            "  2550K .......... .......... .......... .......... .......... 56% 49.7M 0s\n",
            "  2600K .......... .......... .......... .......... .......... 57% 59.2M 0s\n",
            "  2650K .......... .......... .......... .......... .......... 58% 56.5M 0s\n",
            "  2700K .......... .......... .......... .......... .......... 59% 56.5M 0s\n",
            "  2750K .......... .......... .......... .......... .......... 60% 48.5M 0s\n",
            "  2800K .......... .......... .......... .......... .......... 61% 54.0M 0s\n",
            "  2850K .......... .......... .......... .......... .......... 62% 57.2M 0s\n",
            "  2900K .......... .......... .......... .......... .......... 63% 50.0M 0s\n",
            "  2950K .......... .......... .......... .......... .......... 64% 54.6M 0s\n",
            "  3000K .......... .......... .......... .......... .......... 65% 57.1M 0s\n",
            "  3050K .......... .......... .......... .......... .......... 66% 59.1M 0s\n",
            "  3100K .......... .......... .......... .......... .......... 67% 61.6M 0s\n",
            "  3150K .......... .......... .......... .......... .......... 69% 53.8M 0s\n",
            "  3200K .......... .......... .......... .......... .......... 70% 61.8M 0s\n",
            "  3250K .......... .......... .......... .......... .......... 71% 57.7M 0s\n",
            "  3300K .......... .......... .......... .......... .......... 72%  122M 0s\n",
            "  3350K .......... .......... .......... .......... .......... 73% 60.1M 0s\n",
            "  3400K .......... .......... .......... .......... .......... 74% 54.1M 0s\n",
            "  3450K .......... .......... .......... .......... .......... 75% 60.9M 0s\n",
            "  3500K .......... .......... .......... .......... .......... 76% 58.0M 0s\n",
            "  3550K .......... .......... .......... .......... .......... 77% 81.5M 0s\n",
            "  3600K .......... .......... .......... .......... .......... 78%  205M 0s\n",
            "  3650K .......... .......... .......... .......... .......... 79%  251M 0s\n",
            "  3700K .......... .......... .......... .......... .......... 80%  267M 0s\n",
            "  3750K .......... .......... .......... .......... .......... 82%  265M 0s\n",
            "  3800K .......... .......... .......... .......... .......... 83%  235M 0s\n",
            "  3850K .......... .......... .......... .......... .......... 84%  290M 0s\n",
            "  3900K .......... .......... .......... .......... .......... 85%  211M 0s\n",
            "  3950K .......... .......... .......... .......... .......... 86%  281M 0s\n",
            "  4000K .......... .......... .......... .......... .......... 87%  252M 0s\n",
            "  4050K .......... .......... .......... .......... .......... 88%  266M 0s\n",
            "  4100K .......... .......... .......... .......... .......... 89%  232M 0s\n",
            "  4150K .......... .......... .......... .......... .......... 90%  245M 0s\n",
            "  4200K .......... .......... .......... .......... .......... 91%  283M 0s\n",
            "  4250K .......... .......... .......... .......... .......... 92%  259M 0s\n",
            "  4300K .......... .......... .......... .......... .......... 93%  211M 0s\n",
            "  4350K .......... .......... .......... .......... .......... 94%  278M 0s\n",
            "  4400K .......... .......... .......... .......... .......... 96%  267M 0s\n",
            "  4450K .......... .......... .......... .......... .......... 97%  267M 0s\n",
            "  4500K .......... .......... .......... .......... .......... 98%  239M 0s\n",
            "  4550K .......... .......... .......... .......... .......... 99%  266M 0s\n",
            "  4600K .......... .......... .......... ..                   100%  225M=0.1s\n",
            "\n",
            "2023-12-07 11:33:24 (37.9 MB/s) - ‘git-lfs.tar.gz’ saved [4743788/4743788]\n",
            "\n",
            "Executing command: tar xzf git-lfs.tar.gz\n",
            "Output:\n",
            "  None\n",
            "Error:\n",
            "  None\n",
            "\n",
            "Executing command: bash ./install.sh\n",
            "Output:\n",
            " Updated git hooks.\n",
            "Git LFS initialized.\n",
            "\n",
            "Error:\n",
            "  None\n",
            "\n",
            "Executing command: git lfs install\n",
            "Output:\n",
            " Updated git hooks.\n",
            "Git LFS initialized.\n",
            "\n",
            "Error:\n",
            "  None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assuming cwd is github repo\n",
        "directory = r\"\"\"beat_the_bookie/Full_Process/2. Obtaining & Parsing Data/transfer_markt/exported_data\"\"\"\n",
        "\n",
        "utils.save_dict_to_pickle(datasets, directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "211lTs4aBC20",
        "outputId": "233229a4-9e89-48c8-8168-06b88e35f407"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/beat_the_bookie/Full_Process/2. Obtaining & Parsing Data/transfer_markt/exported_data/all_data.pickle'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3UjjfOIC9vI",
        "outputId": "2f6c7848-836b-4b2d-e4cb-4ae06039cf49"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "pwd: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .. && rm -rf beat_the_bookie/"
      ],
      "metadata": {
        "id": "njph_MC6vHM-"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# Needs to get users home directory insted of /content\n",
        "# Needs to take specific directory to save model files into\n",
        "utils = DataManagementUtility(\"/content\", \"PriestlyAdejo\", \"zcemdej@ucl.ac.uk\", \"beat_the_bookie\", \"your_personal_access_token\")\n",
        "\n",
        "# Clone the GitHub repository\n",
        "utils.clone_github_repo()\n",
        "\n",
        "# Save dataframes and metadata\n",
        "datasets = load_all_datasets(all_dataset_names, json_data)  # Assume you have this function defined elsewhere\n",
        "utils.save_dataframes_to_pickle(datasets, 'exported_data')\n",
        "utils.save_metadata_to_file(datasets, 'exported_data')\n",
        "\n",
        "# Get directory sizes and track large files\n",
        "directory_sizes = utility.get_directory_sizes('/content/exported_data')\n",
        "large_files = [size[1] for size in directory_sizes if 'M' in size[0] or 'G' in size[0]]  # Adjust the condition as needed\n",
        "utility.track_large_files(large_files)"
      ],
      "metadata": {
        "id": "73tj69Egewcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting The Data"
      ],
      "metadata": {
        "id": "OvAXCZsIew9_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPe7GdVPWTI7"
      },
      "outputs": [],
      "source": [
        "# Package Installation, Data Importing, Repository Cloning e.t.c\n",
        "! pip install poetry\n",
        "! git clone https://github.com/dcaribou/transfermarkt-datasets.git\n",
        "! cd transfermarkt-datasets && ls && poetry install # && poetry shell"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collecting raw data for 2024 using the api\n",
        "%cd transfermarkt-datasets\n",
        "! make acquire_local ARGS=\"--asset all --season 2023\""
      ],
      "metadata": {
        "id": "qEy2LWg4h-VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make this iterate over all years in a try except to make sure all data is up to date.\n",
        "!make help\n",
        "!make acquire_local ARGS=\"--asset all --season 2023\""
      ],
      "metadata": {
        "id": "B7BWvEeyEzbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!pip install datacompy"
      ],
      "metadata": {
        "id": "X7KDrdztpFqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install dvc\n",
        "!make help\n",
        "!dvc pull"
      ],
      "metadata": {
        "id": "aZ24k1KQwx1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The next cells are to show the difference in the old and new data after it has been retreived\n",
        "# So this is after the data has been gotten form the cloud to show the difference."
      ],
      "metadata": {
        "id": "0dTZxxhUyedA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# retrieve local copies of raw and prepared data from dvc\n",
        "# checkout the readme for instructions about how to gain access\n",
        "\n",
        "# os.system(\"dvc pull\")\n",
        "\n",
        "# Assuming 'transfermarkt-datasets' is a directory at the root in the Colab environment\n",
        "home = \"/content/transfermarkt-datasets\"\n",
        "os.chdir(home)\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")"
      ],
      "metadata": {
        "id": "AgC22o7RsElH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datacompy\n",
        "\n",
        "# Make code to iterate over this dictinonary and print comparisons for each key\n",
        "VALIDATIONS = {\n",
        "    \"appearances\": {\n",
        "        \"joining_keys\": [\"appearance_id\", \"player_id\", \"game_id\"]\n",
        "    },\n",
        "    \"club_games\": {\n",
        "        \"joining_keys\": [\"club_id\", \"opponent_club_id\", \"game_id\"]\n",
        "    },\n",
        "    \"clubs\": {\n",
        "        \"joining_keys\": [\"club_id\", \"domestic_competition_id\"]\n",
        "    },\n",
        "    \"competitions\": {\n",
        "        \"joining_keys\": [\"competition_id\"]\n",
        "    },\n",
        "    \"game_events\": {\n",
        "        \"joining_keys\": [\"game_id\", \"player_id\"]\n",
        "    },\n",
        "    \"game_lineups\": {\n",
        "        \"joining_keys\": [\"game_id\", \"date\"]\n",
        "    },\n",
        "    \"games\": {\n",
        "        \"joining_keys\": [\"game_id\", \"home_club_id\", \"away_club_id\", \"competition_id\"]\n",
        "    },\n",
        "    \"player_valuations\": {\n",
        "        \"joining_keys\": [\"player_id\", \"date\"]\n",
        "    },\n",
        "    \"players\": {\n",
        "        \"joining_keys\": [\"player_id\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "PROJECTPATH = home\n",
        "\n",
        "# Need to change to oldpath\n",
        "OLDPATH = f\"{PROJECTPATH}/data/prep\"\n",
        "NEWPATH = f\"{PROJECTPATH}/data/prep\"\n",
        "\n",
        "def load_old_dataset(asset_name: str) -> pd.DataFrame:\n",
        "    return pd.read_csv(\n",
        "        f\"{OLDPATH}/{asset_name}.csv\"\n",
        "    )\n",
        "\n",
        "def load_new_dataset(asset_name: str) -> pd.DataFrame:\n",
        "    return pd.read_csv(\n",
        "        f\"{NEWPATH}/{asset_name}.csv.gz\"\n",
        "    )\n",
        "\n",
        "def show_datacompy_diff(asset_name: str):\n",
        "\n",
        "    old = load_new_dataset(asset_name)\n",
        "    new = load_new_dataset(asset_name)\n",
        "\n",
        "    comparison = datacompy.Compare(\n",
        "        df1=old,\n",
        "        df2=new,\n",
        "        df1_name=f\"Old Dataset -> For: {asset_name.upper()}\",\n",
        "        df2_name=f\"New Dataset -> For: {asset_name.upper()}\",\n",
        "        join_columns=VALIDATIONS[asset_name][\"joining_keys\"]\n",
        "    )\n",
        "\n",
        "    print(comparison.report())\n",
        "\n",
        "def show_schema_diff(asset_name: str):\n",
        "\n",
        "    old = load_new_dataset(asset_name)\n",
        "    new = load_new_dataset(asset_name)\n",
        "\n",
        "    print(set(old.columns.values) - set(new.columns.values))\n"
      ],
      "metadata": {
        "id": "rL8H1U7NsGG8"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_datacompy_diff(\"appearances\")\n"
      ],
      "metadata": {
        "id": "Te9FxE54zKqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretty print dataset metadata\n",
        "!pip install prettyprint"
      ],
      "metadata": {
        "id": "EzEr3wXe21Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "pN_4OLFF573F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Read the JSON file\n",
        "# Need to also change this location based onthe above old/new configurations\n",
        "dataset_metadata = f\"{home}/data/prep/dataset-metadata.json\"\n",
        "with open(dataset_metadata, 'r') as file:\n",
        "    json_data = json.load(file)\n",
        "\n",
        "# Pretty print the JSON content\n",
        "print(json.dumps(json_data, indent=4))"
      ],
      "metadata": {
        "id": "hPJceaGq5yQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the module\n",
        "# !pip install frictionless\n",
        "from transfermarkt_datasets.core.dataset import Dataset\n",
        "\n",
        "# instantiate the datasets handler\n",
        "td = Dataset()\n",
        "\n",
        "# load all assets into memory as pandas dataframes\n",
        "td.load_assets()\n",
        "\n",
        "# inspect assets\n",
        "td.asset_names # [\"games\", \"players\", ...]\n"
      ],
      "metadata": {
        "id": "UA_2TiHGiVhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the options to display all rows and columns\n",
        "# pd.set_option('display.max_rows', None)  # Replace None with a specific number if you want to limit the rows\n",
        "pd.set_option('display.max_columns', None)  # Replace None with a specific number if you want to limit the columns\n",
        "pd.set_option('display.width', None)  # Adjusts the width of the display to fit all columns if possible\n",
        "pd.set_option('display.max_colwidth', None)  # To see the full content of each cell\n"
      ],
      "metadata": {
        "id": "D1d0Xa5p9s7v"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert schema fields to DataFrame\n",
        "def schema_to_df(schema):\n",
        "    return pd.DataFrame(schema['fields'])\n",
        "\n",
        "# Process each resource in the JSON data\n",
        "def get_schema_df_str(resource, print_data=False):\n",
        "    title = resource['title']\n",
        "    description = resource['description']\n",
        "    schema = resource['schema']\n",
        "\n",
        "    df = schema_to_df(schema)\n",
        "    title_desc = \"\\n\" + title + \"\\n\" + description + \"\\n\"\n",
        "\n",
        "    if print_data:\n",
        "      print(f\"\\n\\nSchema for {title}\\nand Description:{description}\\n\", df)\n",
        "\n",
        "    return df, title_desc\n",
        "\n",
        "# refactor code to use above schemas\n",
        "def load_all_datasets(dataset_names, json_schema):\n",
        "  datasets = {}\n",
        "  dataset_names_sv = [d.split(\"cur_\")[-1] for d in dataset_names] # To get rid of cur_\n",
        "\n",
        "  for name, name_sv in zip(dataset_names, dataset_names_sv):\n",
        "    matching_resource = next((resource for resource in json_schema['resources'] if resource['title'] == name_sv), None)\n",
        "    if matching_resource:\n",
        "      if not name_sv in datasets:\n",
        "        datasets[name_sv] = {}\n",
        "      datasets[name_sv][\"dataset_df\"] = td.assets[name].prep_df # Need to refactor\n",
        "      datasets[name_sv][\"schema_df\"], datasets[name_sv][\"schema_str\"] = get_schema_df_str(matching_resource)\n",
        "  return datasets\n"
      ],
      "metadata": {
        "id": "PYEG5RZXyexp"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a reference to the pandas dataframes containing prepared data\n",
        "all_dataset_names = td.asset_names\n",
        "datasets = load_all_datasets(all_dataset_names, json_data)"
      ],
      "metadata": {
        "id": "iF0uxtjYCt58"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[k for k in datasets]"
      ],
      "metadata": {
        "id": "xFBY7YhuD7wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[\"appearances\"][\"dataset_df\"]"
      ],
      "metadata": {
        "id": "NGq-lH0PD_D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6TWv3ZzAF3Ch"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VcgVTy4XPm-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IZMsZBmbXjiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3TlNZ9JnavIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bD7f9doCarkS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPM+OcdHuTOjlHOVvkKJAe0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}